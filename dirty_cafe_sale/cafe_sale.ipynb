{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405fecaf",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a17dc294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"dirty_cafe_sales.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training\",\n",
    "  file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be6cb2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>item</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_per_unit</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>location</th>\n",
       "      <th>transaction_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>08-09-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_4977031</td>\n",
       "      <td>Cake</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>In-store</td>\n",
       "      <td>16-05-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_4271903</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>19-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_7034554</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27-04-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_3160411</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-store</td>\n",
       "      <td>11-06-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TXN_2602893</td>\n",
       "      <td>Smoothie</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31-03-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TXN_4433211</td>\n",
       "      <td>Cake/Juice</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>06-10-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TXN_6699534</td>\n",
       "      <td>Sandwich</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28-10-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TXN_4717867</td>\n",
       "      <td>Cake/Juice</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>28-07-2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TXN_2064365</td>\n",
       "      <td>Sandwich</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In-store</td>\n",
       "      <td>31-12-2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id        item  quantity  price_per_unit  total_spent  \\\n",
       "0    TXN_1961373      Coffee       2.0             2.0          4.0   \n",
       "1    TXN_4977031        Cake       4.0             3.0         12.0   \n",
       "2    TXN_4271903      Cookie       4.0             1.0          4.0   \n",
       "3    TXN_7034554       Salad       2.0             5.0         10.0   \n",
       "4    TXN_3160411      Coffee       2.0             2.0          4.0   \n",
       "5    TXN_2602893    Smoothie       5.0             4.0         20.0   \n",
       "6    TXN_4433211  Cake/Juice       3.0             3.0          9.0   \n",
       "7    TXN_6699534    Sandwich       4.0             4.0         16.0   \n",
       "8    TXN_4717867  Cake/Juice       5.0             3.0         15.0   \n",
       "9    TXN_2064365    Sandwich       5.0             4.0         20.0   \n",
       "\n",
       "   payment_method  location transaction_date  \n",
       "0     Credit Card  Takeaway       08-09-2023  \n",
       "1            Cash  In-store       16-05-2023  \n",
       "2     Credit Card  In-store       19-07-2023  \n",
       "3             NaN       NaN       27-04-2023  \n",
       "4  Digital Wallet  In-store       11-06-2023  \n",
       "5     Credit Card       NaN       31-03-2023  \n",
       "6             NaN  Takeaway       06-10-2023  \n",
       "7            Cash       NaN       28-10-2023  \n",
       "8             NaN  Takeaway       28-07-2023  \n",
       "9             NaN  In-store       31-12-2023  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running this to get a glimpse of the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cd7ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['transaction_id', 'item', 'quantity', 'price_per_unit', 'total_spent',\n",
      "       'payment_method', 'location', 'transaction_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Standardize column names.\n",
    "# Remove leading/trailing spaces, replace spaces with underscores, and convert to lowercase.\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "\n",
    "# Verify the change\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b402e",
   "metadata": {},
   "source": [
    "# STANDARDIZE 'UNKNOWN','ERRROR' INTO 'NA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72c71554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column: transaction_id\n",
      "Value Counts:\n",
      "transaction_id\n",
      "TXN_9226047    1\n",
      "TXN_8567525    1\n",
      "TXN_4583012    1\n",
      "TXN_6796890    1\n",
      "TXN_9933628    1\n",
      "              ..\n",
      "TXN_3160411    1\n",
      "TXN_7034554    1\n",
      "TXN_4271903    1\n",
      "TXN_4977031    1\n",
      "TXN_1961373    1\n",
      "Name: count, Length: 10000, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: item\n",
      "Value Counts:\n",
      "item\n",
      "Juice       1171\n",
      "Coffee      1165\n",
      "Salad       1148\n",
      "Cake        1139\n",
      "Sandwich    1131\n",
      "Smoothie    1096\n",
      "Cookie      1092\n",
      "Tea         1089\n",
      "UNKNOWN      344\n",
      "NaN          333\n",
      "ERROR        292\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: quantity\n",
      "Value Counts:\n",
      "quantity\n",
      "5          2013\n",
      "2          1974\n",
      "4          1863\n",
      "3          1849\n",
      "1          1822\n",
      "UNKNOWN     171\n",
      "ERROR       170\n",
      "NaN         138\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: price_per_unit\n",
      "Value Counts:\n",
      "price_per_unit\n",
      "3.0        2429\n",
      "4.0        2331\n",
      "2.0        1227\n",
      "5.0        1204\n",
      "1.0        1143\n",
      "1.5        1133\n",
      "ERROR       190\n",
      "NaN         179\n",
      "UNKNOWN     164\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: total_spent\n",
      "Value Counts:\n",
      "total_spent\n",
      "6.0        979\n",
      "12.0       939\n",
      "3.0        930\n",
      "4.0        923\n",
      "20.0       746\n",
      "15.0       734\n",
      "8.0        677\n",
      "10.0       524\n",
      "2.0        497\n",
      "9.0        479\n",
      "5.0        468\n",
      "16.0       444\n",
      "25.0       259\n",
      "7.5        237\n",
      "1.0        232\n",
      "4.5        225\n",
      "1.5        205\n",
      "NaN        173\n",
      "UNKNOWN    165\n",
      "ERROR      164\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: payment_method\n",
      "Value Counts:\n",
      "payment_method\n",
      "NaN               2579\n",
      "Digital Wallet    2291\n",
      "Credit Card       2273\n",
      "Cash              2258\n",
      "ERROR              306\n",
      "UNKNOWN            293\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: location\n",
      "Value Counts:\n",
      "location\n",
      "NaN         3265\n",
      "Takeaway    3022\n",
      "In-store    3017\n",
      "ERROR        358\n",
      "UNKNOWN      338\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      " Column: transaction_date\n",
      "Value Counts:\n",
      "transaction_date\n",
      "UNKNOWN       159\n",
      "NaN           159\n",
      "ERROR         142\n",
      "2023-06-16     40\n",
      "2023-02-06     40\n",
      "             ... \n",
      "2023-11-24     15\n",
      "2023-07-30     15\n",
      "2023-03-11     14\n",
      "2023-07-22     14\n",
      "2023-02-17     14\n",
      "Name: count, Length: 368, dtype: int64\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, we want to investigate data quality issues for each column.\n",
    "# So, we check for each column the unique values and their counts including NaNs.\n",
    "# Loop through every single column in the dataframe\n",
    "# then we read the results to try catch anything unusual.\n",
    "for col in df.columns:\n",
    "    print(f\" Column: {col}\")\n",
    "    \n",
    "    # Show Value Counts including NaNs \n",
    "    print(\"Value Counts:\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "    # This is just separator for readibility\n",
    "    print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa462c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the value counts above, we can see that there are several columns that have \"ERROR\", \"UNKNOWN\" and nan values.\n",
    "# We will standardize these values to be nan for easier handling.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define your mapping rules\n",
    "rules = {\n",
    "    'ERROR': np.nan,\n",
    "    'UNKNOWN': np.nan\n",
    "}\n",
    "\n",
    "# Apply to the entire DataFrame\n",
    "df = df.replace(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c2e97",
   "metadata": {},
   "source": [
    "# FILLNA FOR ITEM COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a599d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2.0\n",
       "4       2.0\n",
       "39      2.0\n",
       "75      2.0\n",
       "80      2.0\n",
       "       ... \n",
       "9966    2.0\n",
       "9969    2.0\n",
       "9990    2.0\n",
       "9995    2.0\n",
       "9997    2.0\n",
       "Name: price_per_unit, Length: 1165, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f we take a look at all rows where item is \"Coffee\", the value for price_per_unit is always 2.0.\n",
    "# Same thing if we take a look at all rows where item is \"Juice\", the value for price_per_unit is always 3.0.\n",
    "# We can see there is consistency in value of price_per_unit for different items\n",
    "# Meaning we can impute missing or unknown items based on the price per unit.\n",
    "df[df['item'] == \"Coffee\"]['price_per_unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14b93d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.0': 'Cookie', '1.5': 'Tea', '2.0': 'Coffee', '3.0': 'Cake/Juice', '4.0': 'Smoothie/Sandwich', '5.0': 'Salad'}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping dictionary from price_per_unit to item\n",
    "price_grouping = (\n",
    "    df[df['item'].notna()]  #  filter out NaN values, we only want to map known items.\n",
    "    .groupby('price_per_unit')['item'] # group by price_per_unit and get the item values. e.g price_per_unit = 2.0 -> [\"Coffee\", \"Coffee\", ...]\n",
    "    .unique() # makes sure each item is only listed once per price bucket. You don't want a dictionary that says 1.0: \"Cookie, Cookie, Cookie...\"\n",
    "    .apply(lambda x: \"/\".join(map(str, x))) # some prices might map to multiple items, so we join them with a slash. e.g 3.0: \"Cake/Juice\"\n",
    "    .to_dict() # convert to dictionary for mapping  \n",
    ")\n",
    "\n",
    "print(price_grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "138bc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before using the mapping, ensure the price_per_unit column is numeric float, as it might have been read as object/string type.\n",
    "import pandas as pd\n",
    "\n",
    "df['price_per_unit'] = pd.to_numeric(df['price_per_unit'], errors='coerce')\n",
    "\n",
    "# Price_per_unit is float, but we also need to make sure the keys in the mapping dictionary 'price_grouping' are also float.\n",
    "# So we convert them.\n",
    "clean_mapping = {float(k): v for k, v in price_grouping.items()}\n",
    "\n",
    "# Now we can use this mapping to fill in the missing item values.\n",
    "df['item'] = df['item'].fillna(df['price_per_unit'].map(clean_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "360396ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item is Nan X price_per_unit notNan count: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's verify our fillna, we have two conditions : item is naN AND price_per_unit is NOT NaN. filter this should give us 0 rows.\n",
    "# We should still have Nan for item if price_per_unit is also NaN.\n",
    "missing = df['item'].isna() & df['price_per_unit'].notna()\n",
    "\n",
    "print(f\"Item is Nan X price_per_unit notNan count: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e93b1",
   "metadata": {},
   "source": [
    "# FILLNA FOR QUANTITY, PRICE PER UNIT, TOTAL SPENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f533b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next,for 3 columns, quantity, price_per_unit, total_price is connected via a formula:\n",
    "# total_price = quantity * price_per_unit\n",
    "# So we can use this formula to impute missing values in any of these 3 columns\n",
    "# We will create a function on condition that only one of the 3 columns is missing, we can calculate it from the other two.\n",
    "# If more than one is missing, we cannot impute it.\n",
    "\n",
    "# Define the columns we are working with\n",
    "# Purposely using col2 to avoid confusion with col defined earlier\n",
    "col2 = ['quantity', 'price_per_unit', 'total_spent']\n",
    "\n",
    "# Convert string columns to float\n",
    "for c in col2:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Create a mask for rows where exactly ONE value is missing\n",
    "mask_one_missing = df[col2].isnull().sum(axis=1) == 1\n",
    "\n",
    "\n",
    "# Calculate Missing 'total_spent'\n",
    "df.loc[mask_one_missing & df['total_spent'].isnull(), # filter rows with one missing and total_spent is missing\n",
    "    'total_spent'] = df['quantity'] * df['price_per_unit'] # then use formula\n",
    "\n",
    "#  Calculate Missing 'quantity'\n",
    "df.loc[mask_one_missing & df['quantity'].isnull(), 'quantity'] = \\\n",
    "    df['total_spent'] / df['price_per_unit']\n",
    "\n",
    "# Calculate Missing 'price_per_unit'\n",
    "df.loc[mask_one_missing & df['price_per_unit'].isnull(), 'price_per_unit'] = \\\n",
    "    df['total_spent'] / df['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b20a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with exactly one missing value among ['quantity', 'price_per_unit', 'total_spent']: 1398\n"
     ]
    }
   ],
   "source": [
    "# Lets verify this\n",
    "# 'col2' and 'mask_one_missing' are already defined above\n",
    "# This should produce 0\n",
    "\n",
    "print(f\"Rows with exactly one missing value among {col2}: {mask_one_missing.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e99c6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    08-09-2023\n",
      "1    16-05-2023\n",
      "2    19-07-2023\n",
      "3    27-04-2023\n",
      "4    11-06-2023\n",
      "Name: transaction_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lastly, we standardize the transaction_date column to dd-mm-yyyy format. just because we can.\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# Verify the change\n",
    "print(df['transaction_date'].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
